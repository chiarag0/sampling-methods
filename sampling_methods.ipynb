{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc058a90-6c2d-452d-96b4-24ec3836920b",
   "metadata": {},
   "source": [
    "# Sampling Methods\n",
    "\n",
    "In Machine Learning, we often deal with *complex probability distributions*, which require us to estimate expectations, compute difficult integrals, and draw samples. For many probabilistic models of practical interest we find out that analytical solutions are intractable, and we have to resort to *approximations*: in this context, sampling methods provide a powerful set of tools to approximate such computations through random sampling.\n",
    "\n",
    "In many applications, we are not directly interested in the posterior distribution itself, but rather in using it to compute expectations, such as making predictions. Formally, we want to calculate the expected value of some function $f(z)$ under a probability distribution $p(z)$. This expectation is generally expressed as an integral for continuous variables (or a sum, for discrete variables): \n",
    "$$ E[f] = ∫ f(z) p(z) dz$$ \n",
    "As we said, in many cases this integral is too complex to compute analytically. This is where sampling methods come to our aid: the idea is to approximate this expectation by generating a set of *samples* $ L = z^{(1)}, ..., z^{(L)}$ drawn independently from the distribution $p(z)$, and then estimate the expectation as a simple average of these values: \n",
    "$$ \\hat{f} = \\frac{1}{L} ∑_{l=1}^L f(z^{(l)}) $$\n",
    "This estimator is *unbiased*, meaning that its expected value equals the true expectation (i.e. $E[\\hat{f}] = E[f]$) and its variance is given by:\n",
    "$$var[\\hat{f}] = \\frac{1}{L} E[(f - E[f])^2]$$\n",
    "Note that the accuracy of the estimator does not depend on the dimensionality of $z$, which means that, in practice, sometimes even 10 or 20 independent samples can lead to a sufficiently accurate result.\n",
    "\n",
    "However, there are some challenges:\n",
    "- The samples ${z^{(l)}}$ may not be independent, which reduces the real sample size.\n",
    "- If $f(z)$ is small in regions where $p(z)$ is large, and vice versa, then the estimate may be dominated by regions of small probability (rare events), requiring many more samples to achieve sufficient accuracy.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e77dd42-6af2-42bd-9ab8-1c1b95d7a54d",
   "metadata": {},
   "source": [
    "## 1 - Basic Sampling Algorithms\n",
    "In this section, we consider some simple methods for generating random samples\n",
    "from a given distribution, assuming that we're using an algorithm that generates pseudo-random numbers distributed uniformly over $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa877ffd-8330-44f2-8370-6ef5a667cdce",
   "metadata": {},
   "source": [
    "### 1.1 - Standard Distributions\n",
    "\n",
    "Let's start with discussing how to generate random numbers from simple nonuniform distributions.  \n",
    "\n",
    "Suppose that $z$ is uniformly distributed over the interval $(0, 1)$,\n",
    "we define a transformation function $f(·)$ such that $y = f(z)$ and the resulting \n",
    "$y$ values follow a specific desired distribution $p(y)$.\n",
    "\n",
    "The relationship between the distributions of $z$ and $y$ is given by the formula $$p(y) = p(z)  |\\frac{dz}{dy}| $$ where, in this case, $p(z)=1$, since $z∼Uniform(0,1)$.\n",
    "\n",
    "Integrating $p(y)$, we obtain: $$z = h(y) = ∫_{-∞}^{y} p(\\hat{y}) d\\hat{y}$$\n",
    "\n",
    "Calculating this indefinite integral gives us a function $h(y)$, which maps a value of $y$ to a uniform value $z$. \n",
    "\n",
    "To sample from $p(y)$, we invert this function: $$y = h^{-1}(z)$$\n",
    "\n",
    "In other words, we have to transform the uniformly distributed random numbers $z$ using a function which is the inverse of the indefinite integral of the desired distribution.\n",
    "\n",
    "This method works well when $h^{-1}$ is easy to compute; if that isn't the case, we need to rely on other techniques, such as the ones introduced in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f20de-6325-42d6-964d-185f7ad8ec2c",
   "metadata": {},
   "source": [
    "#### In Practice\n",
    "The graphs in this section show the results of sampling from three widely used probability distributions: the Normal, Exponential and Beta distributions. Although NumPy provides built-in sampling functions, the function `inverse_transform_sample` defined below chooses a less refined approach by following step by step the process described in our last paragraph, from scratch. For each distribution, we perform Inverse Transform Sampling and visualize the sampled data along with the theoretical PDF.\n",
    "\n",
    "Using Jupyter's interactive widgets (see [documentation](https://ipywidgets.readthedocs.io/en/latest/examples/Using%20Interact.html)) we can  experiment with different distribution parameters and observe how the sampled data behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36e5cdad-5980-47ef-b175-c68ceb251114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm, expon, beta\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from scipy.interpolate import interp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e98a65-d774-44b8-a04e-cb0202457319",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_sample(pdf, z_min, z_max, n_samples=1000):\n",
    "\n",
    "    z = np.linspace(z_min, z_max, 10000)\n",
    "    pdf = pdf(z)  # Evaluate PDF in each z\n",
    "    dz = z[1] - z[0]  # Distance between two consecutive values, to approximate the PDF integral with a sum\n",
    "    cdf = np.cumsum(pdf) * dz  # Evaluate CDF using cumulative sum\n",
    "    cdf = cdf / cdf[-1]  # Normalize to [0,1]\n",
    "\n",
    "    # Compute the inverse using ScyPi's linear interpolation.\n",
    "    # To avoid ValueErrors being raised, set 'bounds_error' to False\n",
    "    # So that out of bounds values are assigned 'fill_value', i.e. the extreme values of the CDF\n",
    "    inv_cdf = interp1d(cdf, z, bounds_error=False, fill_value=(z[0], z[-1]))\n",
    "\n",
    "    # Generate n_samples uniform random values in (0, 1)\n",
    "    u = np.random.uniform(0, 1, size=n_samples) \n",
    "    \n",
    "    return inv_cdf(u)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "76d12d2f-8a5a-4abc-9531-2d924adee9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every time the parameters are changed from the widget, \n",
    "# 'plot' is called with the updated values\n",
    "def plot_normal():\n",
    "    @widgets.interact(mu=widgets.FloatSlider(value=0, min=-5, max=5, step=0.1 ,description='μ'),\n",
    "                      sigma=widgets.FloatSlider(value=1, min=0.1, max=5, step=0.1, description='σ'),\n",
    "                      n_samples=widgets.IntSlider(value=5000, min=100, max=10000, step=100, description='# samples'))\n",
    "    def plot(mu, sigma, n_samples):\n",
    "\n",
    "        # Set the approximated interval in which the PDF and the CDF will be evaluated \n",
    "        z_min = mu - 5*sigma\n",
    "        z_max = mu + 5*sigma\n",
    "\n",
    "        # Lambda function to define the PDF for a normal distribution\n",
    "        # with current mean (mu) and standard deviation (sigma)\n",
    "        pdf = lambda z: norm.pdf(z, loc=mu, scale=sigma)\n",
    "\n",
    "        # Generate samples from the interval\n",
    "        samples = inverse_transform_sample(pdf, z_min, z_max, n_samples)\n",
    "\n",
    "        # Plotting code generated by ChatGPT\n",
    "        z = np.linspace(z_min, z_max, 1000)\n",
    " \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.hist(samples, bins='auto', density=True, alpha=0.6, label='Sampled')\n",
    "        plt.plot(z, pdf(z), 'r-', label=f'N({mu}, {sigma}²)')\n",
    "        plt.title(\"Inverse Transform Sampling: Normal Distribution\")\n",
    "        plt.xlabel(\"z\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def plot_exponential():\n",
    "    @widgets.interact(lmbda=widgets.FloatSlider(value=1.0, min=0.1, max=5.0, step=0.1, description='λ'),\n",
    "                      n_samples=widgets.IntSlider(value=5000, min=100, max=10000, step=100, description='# samples'))\n",
    "    def plot(lmbda, n_samples):\n",
    "        z_min = 0\n",
    "        z_max = 10\n",
    "        \n",
    "        # Lambda function to define the PDF for an exponential distribution\n",
    "        pdf = lambda z: expon.pdf(z, scale=1/lmbda)\n",
    "        samples = inverse_transform_sample(pdf, z_min, z_max, n_samples)\n",
    "        \n",
    "        z = np.linspace(z_min, z_max, 1000)\n",
    "        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.hist(samples, bins='auto', density=True, alpha=0.6, label='Sampled')\n",
    "        plt.plot(z, pdf(z), 'r-', label=f'Exp(λ={lmbda})')\n",
    "        plt.title(\"Inverse Transform Sampling: Exponential Distribution\")\n",
    "        plt.xlabel(\"z\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def plot_beta():\n",
    "    @widgets.interact(a=widgets.FloatSlider(value=2.0, min=0.1, max=10.0, step=0.1),\n",
    "                      b=widgets.FloatSlider(value=2.0, min=0.1, max=10.0, step=0.1),\n",
    "                      n_samples=widgets.IntSlider(value=5000, min=100, max=10000, step=100, description='# samples'))\n",
    "    def plot(a, b, n_samples):\n",
    "        z_min = 0\n",
    "        z_max = 1\n",
    "        \n",
    "        # Lambda function to define the PDF for a beta distribution\n",
    "        pdf = lambda z: beta.pdf(z, a, b)\n",
    "        samples = inverse_transform_sample(pdf, z_min, z_max, n_samples)\n",
    "        \n",
    "        z = np.linspace(z_min, z_max, 1000)\n",
    "        \n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.hist(samples, bins='auto', density=True, alpha=0.6, label='Sampled')\n",
    "        plt.plot(z, pdf(z), 'r-', label=f'Beta({a}, {b})')\n",
    "        plt.title(\"Inverse Transform Sampling: Beta Distribution\")\n",
    "        plt.xlabel(\"z\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d2a1b90-ce1e-4881-8b5e-e60ed37f24eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3759d7a84b7d43149c241fa79bd6d57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='μ', max=5.0, min=-5.0), FloatSlider(value=1.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_normal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06709bd-13f6-48a4-b035-abd2d01326ce",
   "metadata": {},
   "source": [
    "We can observe how changing the mean (μ) and standard deviation (σ) affects the shape and position of the distribution. Increasing the mean shifts the bell curve to the right, while decreasing it moves it to the left. Instead, varying the standard deviation affects the spread: a larger $σ$ results in a wider and flatter bell curve, while a smaller $σ$ makes it narrower and sharper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b65cc29-50ca-4279-a93e-93352e91e872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee1e117646b41b082da20efac85157b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='λ', max=5.0, min=0.1), IntSlider(value=5000, descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_exponential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b6548c-e456-4a98-87ad-ab9cf17c5acb",
   "metadata": {},
   "source": [
    "In the Exponential distribution plot, the parameter $λ$ controls how rapidly the distribution decays: a high $λ$ concentrates the probability near zero, while a smaller $λ$ flattens the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0ea858f-56ca-4d8f-a05a-fac44e125066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b095a01bcb141fd89070dab7518a560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.0, description='a', max=10.0, min=0.1), FloatSlider(value=2.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_beta()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0122d-08e9-47c5-b590-98ff67d77a68",
   "metadata": {},
   "source": [
    "We can control the behaviour of a Beta distribution by varying its parameters $a$ and $b$ : when both are equal and greater than 1, the distribution is symmetric and bell-shaped around 0.5 ; when $a > b$, the distribution skews toward 1, but when $b > a$, it skews toward 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e8e341-2ad3-4ff2-ac74-aa32aa1585d6",
   "metadata": {},
   "source": [
    "Across all three distributions, the number of samples `n_samples` has a big impact on the accuracy of the histograms. With a low number of samples, the histograms appear irregular and does not match the PDF properly in many areas of the graph. As the number of samples increases, the histogram becomes smoother and clearly approximates the theoretical curve more closely. This effect is a direct consequence of the law of large numbers, which ensures that empirical distributions converge to their true forms as the sample size grows. "
   ]
  },
  {
   "attachments": {
    "48e6197f-682c-4dab-9811-cee72dc481aa.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a721bb08-699e-4f53-8594-5cb88a9889a9",
   "metadata": {},
   "source": [
    "### 1.2 - Rejection Sampling\n",
    "Rejection Sampling is a technique that allows us to generate samples from complex probability distributions.\n",
    "\n",
    "Let's consider a univariate distribution $p(z)$: suppose that it is not a standard distribution and that sampling directly from it would be difficult. However, we can compute the value of $p(z)$ for any $z$, up to a normalization constant $Z$: $$p(z) = \\frac{1}{Z_p} \\tilde{p}(z)$$ where $\\tilde{p}(z)$ can be easily evaluated, but $Z_p$ is unknown.\n",
    "\n",
    "To proceed, we pick a simpler distribution $q(z)$, called the *proposal distribution*, from which we can easily draw samples. \n",
    "\n",
    "Next, we select a constant $k$, chosen such that $kq(z) \\geq \\tilde{p}(z)$ $\\forall z$. The scaled function $kq(z)$ is called the *comparison function*.\n",
    "\n",
    "The sampling process works as follows:\n",
    "1. Generate a random number $z_0$ from the distribution $q(z)$\n",
    "2. Generate a random number $u_0$ from the uniform distribution over $[0, kq(z_0)]$\n",
    "3. If $u_0 > \\tilde{p}(z_0)$ the sample is rejected, otherwise it is retained\n",
    "\n",
    "![Screenshot 2025-05-06 102544.png](attachment:48e6197f-682c-4dab-9811-cee72dc481aa.png)\n",
    "\n",
    "<sub> Figure taken from \"Pattern Recognition and Machine Learning\" (Christopher M. Bishop) </sub>\n",
    "\n",
    "With reference to the figure above, the samples are rejected if they fall in the grey area between the unnormalized distribution $\\tilde{p}(z)$ and the scaled distribution $kq(z)$. The accepted samples have uniform distribution under the curve of $\\tilde{p}(z)$ and the corresponding values of $z$ will be distributed according to the desired target distribution $p(z)$.\n",
    "\n",
    "The values of $z$, which are drawn from the proposal distribution $q(z)$ are accepted with probability $$\\frac{\\tilde{p}(z)}{kq(z)}$$ So, the overall probability of accepting a sample is: $$p(accept) = ∫ \\frac{\\tilde{p}(z)}{kq(z)} q(z) dz\n",
    "          = \\frac{1}{k} ∫ \\tilde{p}(z) dz$$\n",
    "\n",
    "Intuitively, the acceptance rate depends on the ratio between the area under the curve of the unnormalized distribution $\\tilde{p}(z)$ and the area under the curve of the comparison function $kq(z)$: this tells us that the constant $k$ should be the smallest value satisfying $kq(z) \\geq \\tilde{p}(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dbf66b-b4bd-450a-946e-dd6a226087f0",
   "metadata": {},
   "source": [
    "#### In Practice \n",
    "As a practical illustration of Rejection Sampling, we will consider the implementation of the example proposed by Section 11.2.2 of Bishop's \"Pattern Recognition and Machine Learning\", i.e. the task of drawing samples from a Gamma distribution, defined as:\n",
    "$$Gam(z|a, b) = \\frac{b^a z^{a-1} exp(-bz)}{\\Gamma(a)} $$\n",
    "When $a>1$, the Gamma distribution has a bell-shaped form. A suitable choice for the proposal distribution is the Cauchy distribution, because it is also bell-shaped and can be sampled efficiently. We use a generalized form of the Cauchy, obtained by transforming a uniform random variable $y$ using $z = b \\tan{y} + c$, which gives random numbers distributed according to $$q(z)=\\frac{k}{1+(\\frac{z-c}{b})^2} $$\n",
    "As we said, the rejection rate is minimized by choosing the smallest $k$ satisfying $kq(z) \\geq \\tilde{p}(z)$ and, in particular, by setting $c = a − 1$ and $b^2 = 2a − 1$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71fe0895-6db4-4820-ba29-fc4a5ed9bb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494841067189454cb92f3dc23c8b67fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=3.0, description='a', max=10.0, min=1.1), FloatSlider(value=1.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "# Base code generated by ChatGPT and edited (and commented)\n",
    "def rejection_sampling_gamma(a=3.0, b=1.0, k=4.0, n_samples=10000):\n",
    "\n",
    "    # Parameters set according to Bishop's example\n",
    "    c = a - 1\n",
    "    bc = np.sqrt(2 * a - 1)\n",
    "\n",
    "    z_vals = []   # Candidate values\n",
    "    u_vals = []   # Uniform values generated for comparison\n",
    "    accepted_mask = []   # Boolean values indicating if each z was accepted\n",
    "    accepted_samples = []   # List of accepted values\n",
    "\n",
    "\n",
    "    # Iterate until n_samples values have been accepted\n",
    "    while len(accepted_samples) < n_samples:\n",
    "\n",
    "        # Generate a sample z from a Cauchy distribution centered in c and with scale bc\n",
    "        # (Through inversion sampling)\n",
    "        y = np.random.uniform(-np.pi / 2, np.pi / 2)\n",
    "        z = bc * np.tan(y) + c\n",
    "\n",
    "        # Gamma is defined by z > 0 -> negative samples are rejected\n",
    "        if z <= 0:\n",
    "            continue\n",
    "\n",
    "        q_z = 1 / (np.pi * bc * (1 + ((z - c) / bc) ** 2))  # Value of the Cauchy proposal density\n",
    "        p_z = z ** (a - 1) * np.exp(-b * z)  # Unnormalized Gamma target\n",
    "\n",
    "        u = np.random.uniform(0, k * q_z) # Draw u from a uniform distribution between 0 and kq(z)\n",
    "        accept = u < p_z  # Accept or reject sample\n",
    "\n",
    "        # Store values for analysis and visualization\n",
    "        z_vals.append(z)\n",
    "        u_vals.append(u)\n",
    "        accepted_mask.append(accept)\n",
    "        if accept:\n",
    "            accepted_samples.append(z)\n",
    "\n",
    "    # Convert lists to NumPy arrays for easier handling \n",
    "    z_vals = np.array(z_vals)\n",
    "    u_vals = np.array(u_vals)\n",
    "    accepted_mask = np.array(accepted_mask)\n",
    "    accepted_samples = np.array(accepted_samples)\n",
    "\n",
    "    # 90th percentile of proposed values to avoid long tails and limit plot range\n",
    "    z_max = np.percentile(z_vals, 90)\n",
    "\n",
    "    # NumPy array containing containing 1000 equidistant values in 0.001 and z_max\n",
    "    # x are the values over which we'll evaluate the functions\n",
    "    x = np.linspace(0.001, z_max, 1000)\n",
    "\n",
    "    p = x ** (a - 1) * np.exp(-b * x)  # Unnormalized Gamma target function\n",
    "\n",
    "    q = 1 / (np.pi * bc * (1 + ((x - c) / bc) ** 2))  # Cauchy proposal function\n",
    "    q_scaled = k * q  # proposal function scaled by k \n",
    "\n",
    "    # Histogram of accepted samples\n",
    "    counts, bins = np.histogram(accepted_samples, bins=80, range=(0, z_max))\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])\n",
    "    bin_width = bins[1] - bins[0]\n",
    "    scale = np.trapz(p, x) / (len(accepted_samples) * bin_width) # To match the histogram with the unnormalized density p\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "\n",
    "    # First subplot: histogram + target + proposal\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(bin_centers, counts * scale, width=bin_width, alpha=0.6, label='Sampled')\n",
    "    plt.plot(x, p, label=r'Target $\\tilde{p}(z)$', color='blue')\n",
    "    plt.plot(x, q_scaled, label=r'Scaled Proposal $kq(z)$', color='red', linestyle='--')\n",
    "    plt.xlim(0, z_max)\n",
    "    plt.xlabel(\"z\")\n",
    "    plt.ylabel(\"Density (scaled)\")\n",
    "    plt.title(f\"Rejection Sampling\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Second plot: accepted vs rejected\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(x, p, label=r'Target $\\tilde{p}(z)$', color='blue')\n",
    "    plt.plot(x, q_scaled, label=r'Scaled Proposal $kq(z)$', color='red', linestyle='--')\n",
    "    mask = z_vals <= z_max\n",
    "    plt.scatter(z_vals[mask & ~accepted_mask], u_vals[mask & ~accepted_mask],\n",
    "                color='red', alpha=0.4, s=10, label='Rejected')\n",
    "    plt.scatter(z_vals[mask & accepted_mask], u_vals[mask & accepted_mask],\n",
    "                color='green', alpha=0.4, s=10, label='Accepted')\n",
    "    plt.xlim(0, z_max)\n",
    "    plt.ylim(0, np.max(u_vals[mask]) * 1.1)\n",
    "    plt.xlabel(\"z\")\n",
    "    plt.ylabel(\"u\")\n",
    "    plt.title(\"Accepted vs Rejected Samples\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget\n",
    "interact(rejection_sampling_gamma,\n",
    "         a=FloatSlider(value=3.0, min=1.1, max=10.0, step=0.1, description='a'),\n",
    "         b=FloatSlider(value=1.0, min=0.1, max=5.0, step=0.1, description='b'),\n",
    "         k=FloatSlider(value=4.0, min=1.0, max=10.0, step=0.1, description='k'),\n",
    "         n_samples=IntSlider(value=10000, min=100, max=50000, step=100, description='n_samples'));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81080a8-fe4e-45e9-8b07-6bfc4cd1c057",
   "metadata": {},
   "source": [
    "As we can see from the graphs, varying the parameters $a$, $b$, $k$ and `n_samples` significantly affects the behavior of the Rejection Sampling process. \n",
    "\n",
    "The parameters $a$ and $b$ define the shape and scale of the target Gamma distribution and are set by default to two values satisfying the conditions that minimize the rejection rate. If they are altered, the changes affect how well the proposal distribution approximates the target, which clearly worsens the acceptance rate.\n",
    "\n",
    "The parameter $k$ controls the vertical scaling of the proposal distribution. A larger $k$ increases the area between the scaled proposal curve and the target curve, which lowers the acceptance rate and the overall efficiency.\n",
    "\n",
    "Finally, increasing `n_samples` requests more accepted samples before interrupting the sampling process: the final estimate will be closer to the target distribution, but, clearly, the computational cost will be higher."
   ]
  },
  {
   "attachments": {
    "1e1f4575-76ee-4ba2-a2fc-b743e4aa849f.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c258eb99-edd8-4f5e-ba7f-c9fabafaae93",
   "metadata": {},
   "source": [
    "### 1.3 - Adaptive Rejection Sampling\n",
    "Often, defining a good proposal distribution $q(z)$ in advance is hard and applying Rejection Sampling is not feasible. Adaptive Rejection Sampling is an alternative approach in which, instead of defining the envelope function analytically, the envelope is built *dynamically*, using actual evaluations of the target distribution $p(z)$.\n",
    "\n",
    "This method is especially effective when $p(z)$ is *log-concave*, meaning that $ln$ $p(z)$ is a concave function (in other words, it has derivatives that are nonincreasing functions of $z$).\n",
    "\n",
    "The process works as follows:\n",
    "1. Evaluate the function $ln$ $p(z)$ and its gradient at some initial set of grid points\n",
    "2. At each of these points, compute the tangent lines to $ln$ $p(z)$\n",
    "3. Use the intersections of the tangent lines to construct the envelope function\n",
    "4. Draw a sample from this envelope instead of from $q(z)$\n",
    "5. Apply the rejection sampling criterion to decide whether to accept or reject it:\n",
    "   - If the sample is accepted, it’s treated as a valid sample from the target distribution\n",
    "   - If the sample is rejected, it's added to the set of grid points and used to refine the envelope distribution (a new tangent line is computed at this point and the envelope is updated)\n",
    "\n",
    "As the number of grid points increases, the envelope function becomes a better approximation of the desired distribution $p(z)$ and the probability of rejecting a sample decreases over time.  \n",
    "\n",
    "![Screenshot 2025-05-20 113358.png](attachment:1e1f4575-76ee-4ba2-a2fc-b743e4aa849f.png)\n",
    "\n",
    "<sub> Figure taken from \"Pattern Recognition and Machine Learning\" (Christopher M. Bishop) </sub>\n",
    "\n",
    "The Adaptive Rejection Sampling method can also be extended to distributions that are not log-concave by following each Rejection Sampling step with a Metropolis-Hastings step (Paragraph 2.2).\n",
    "\n",
    "Clearly, to minimize the number of rejected samples and increase efficiency, the envelope should be a close match of the distribution function.\n",
    "\n",
    "Note that ARS is not suited for problems of high dimensionality: the acceptance rate decreases exponentially when dimensionality increases, which is a generic feature of rejection sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30fdf8-c9eb-4996-a63a-4e2888639d05",
   "metadata": {},
   "source": [
    "#### In Practice\n",
    "The next example demonstrates how Adaptive Rejection Sampling works when sampling from a Gaussian distribution, which is a classic example of a log-concave distribution. We make use of the `ARSpy` library (see [documentation](https://arspy.readthedocs.io/en/latest/#)) and use an interactive widget to modify distribution parameters and visualize the target density, log-density, sampled histogram and the envelope function used by the algorithm.\n",
    "\n",
    "> ##### Note:  \n",
    "> As it is quite dated, the ARSpy library couldn't be installed via pip due to incompatibility issues. To make this piece of code work, follow these steps:\n",
    "> - Manually download the library from [PyPI](https://pypi.org/project/ARSpy/)\n",
    "> - Extract the contents and navigate to the sub-folder `arspy`\n",
    "> - Copy it directly into the same directory as this Jupyter Notebook (be careful to copy it as it is and not nested inside of other folders)\n",
    "\n",
    "As we know, the pdf of a Gaussian distribution is \n",
    "$$ p(z) = \\frac{1}{\\sqrt{2 \\pi \\sigma}} \\exp{(-\\frac{1}{2}(\\frac{z-\\mu}{\\sigma})^2)} $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3928d714-2dcc-43b3-94fb-d80f85b8d52a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ef528eb4bd4ef4a2a718cffe3039f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='μ', max=3.0, min=-3.0), FloatSlider(value=2.0, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_ars(mu=0.0, sigma=1.0, n_support=5, n_samples=10000)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from arspy.ars import adaptive_rejection_sampling\n",
    "from ipywidgets import interact, FloatSlider\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "# log-pdf for a Gaussian\n",
    "def gaussian_logpdf(z, mu=0.0, sigma=2.0):\n",
    "    return -0.5 * ((z - mu) / sigma) ** 2 - np.log(np.sqrt(2 * np.pi) * sigma)\n",
    "    \n",
    "# log-pdf's derivative\n",
    "def gaussian_logpdf_derivative(z, mu=0.0, sigma=1.0):\n",
    "    return -(z - mu) / (sigma ** 2)\n",
    "\n",
    "def plot_ars(mu=0.0, sigma=1.0, n_support=5, n_samples=10000):\n",
    "    # ARS sampling\n",
    "    # The initial interval [a, b] is set to two standard deviations around the mean\n",
    "    # As stated in ars.py, the derivative of log-pdf is not required as input to the sampler\n",
    "    samples = adaptive_rejection_sampling(\n",
    "        logpdf=lambda z: gaussian_logpdf(z, mu, sigma),\n",
    "        a=mu - 2 * sigma, \n",
    "        b=mu + 2 * sigma,\n",
    "        domain=(float(\"-inf\"), float(\"inf\")),\n",
    "        n_samples=n_samples,\n",
    "        seed=37\n",
    "    )\n",
    "\n",
    "    x = np.linspace(-8, 8, 500)\n",
    "    \n",
    "    # Compute the values of the Gaussian PDF\n",
    "    pdf_vals = norm.pdf(x, loc=mu, scale=sigma)\n",
    "    \n",
    "    # Compute the corresponding log-PDF values\n",
    "    logpdf_vals = gaussian_logpdf(x, mu, sigma)\n",
    "\n",
    "\n",
    "    # Create 2 side by side subplots\n",
    "    fig, (ax_pdf, ax_logpdf) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram of ARS samples + Gaussian PDF\n",
    "    ax_pdf.hist(samples, bins=50, density=True, alpha=0.6, label='ARS Samples')\n",
    "    ax_pdf.plot(x, norm.pdf(x, mu, sigma), 'r', label='Gaussian PDF')\n",
    "    ax_pdf.set_title(\"Adaptive Rejection Sampling\")\n",
    "    ax_pdf.set_xlabel(\"z\")\n",
    "    ax_pdf.set_ylabel(\"Density\")\n",
    "    ax_pdf.legend()\n",
    "    ax_pdf.grid(True)\n",
    "    \n",
    "    # Plot log-pdf and example of starting envelope\n",
    "    # Build envelope with tangents \n",
    "    support_points = np.linspace(mu - 2*sigma, mu + 2*sigma, n_support)\n",
    "    tangents = [gaussian_logpdf_derivative(s, mu, sigma) for s in support_points]\n",
    "    logpdf_at_support = [gaussian_logpdf(s, mu, sigma) for s in support_points]\n",
    "\n",
    "    envelope_lines = []\n",
    "    # Compute tangents at every support point and store them\n",
    "    for s, m, h in zip(support_points, tangents, logpdf_at_support):\n",
    "        envelope_lines.append(h + m * (x - s))\n",
    "    # Compute the upper envelope by comparing the arrays and selecting the minimum element-wise  \n",
    "    envelope = np.minimum.reduce(envelope_lines)\n",
    "    \n",
    "    ax_logpdf.plot(x, gaussian_logpdf(x, mu, sigma), 'g--', label='log-PDF')\n",
    "    ax_logpdf.plot(x, envelope, 'b-', label='Envelope')\n",
    "    ax_logpdf.set_title(\"log-PDF + example of starting envelope\")\n",
    "    ax_logpdf.set_xlabel(\"z\")\n",
    "    ax_logpdf.set_ylabel(\"log-pdf\")\n",
    "    ax_logpdf.legend()\n",
    "    ax_logpdf.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget\n",
    "interact(plot_ars,\n",
    "         mu=FloatSlider(value=0.0, min=-3.0, max=3.0, step=0.1, description='μ'),\n",
    "         sigma=FloatSlider(value=2.0, min=0.3, max=3.0, step=0.1, description='σ'),\n",
    "         n_support=IntSlider(value=5, min=3, max=15, step=1, description='# supports'),\n",
    "         n_samples=IntSlider(value=10000, min=500, max=20000, step=500, description='# samples'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f9cc0-f4a7-4041-bdae-2b807638b3d6",
   "metadata": {},
   "source": [
    "The plot on the left contains a histogram of the samples generated using ARS alongside the true Gaussian PDF, showing the quality of the approximation. \n",
    "\n",
    "The plot on the right displays the log of the target density, together with its initial upper envelope, formed by the tangents at selected support points.\n",
    "\n",
    "As we know, $\\mu$ and $\\sigma$ represent the mean and standard deviation of the Gaussian distribution, and changing them shifts and scales the PDF and log-PDF curves accordingly, while the envelops adapts to the new shape of the log-PDF.\n",
    "\n",
    "Changing the number of the initial supporting points used to build the tangents that form the envelope results in tighter or looser envelopes, showing how more support points lead to a more precise approximation, which consequently increases sampling efficiency.\n",
    "\n",
    "As before, increasing the number of samples makes the histogram smoother and provides a more accurate estimate of the target PDF, at the expense of a higher computational cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288cdb62-5344-4fd0-b96f-754f1e5cdf72",
   "metadata": {},
   "source": [
    "### 1.4 - Importance Sampling\n",
    "We already discussed in Paragraph 1 the reasoning behind wanting to calculate the expected value of some function $f(z)$ under a probability distribution $p(z)$, expressed as: \n",
    "$$ E[f] = ∫ f(z) p(z) dz$$ \n",
    "This integral may be too difficult to compute, which is why we would like to draw samples from $p(z)$ and then estimate the expectation as the average of these values: but, as we said when introducing Rejection Sampling, if $p(z)$ is not a standard distribution, sampling directly from it would be too complex. \n",
    "\n",
    "We could discretize $z$-space and use a uniform grid to evaluate the function across the space; however, this approach scales poorly in high dimensions.\n",
    "\n",
    "As in the case of Rejection Sampling, Importance Sampling is based on the use\n",
    "of a proposal distribution $q(z)$, chosen to be similar to $p(z)$, from which it's easy to draw samples. \n",
    "\n",
    "We can then express the expectation in the form of a finite sum over samples ${z^{(l)}\n",
    "}$ drawn from $q(z)$: \n",
    "$$ E[f] = ∫ f(z) p(z) dz = ∫ f(z) \\frac{p(z)}{q(z)} q(z) dz \\simeq \\frac{1}{L} \\sum_{l=1}^{L} \\frac{p(z^{(l)})}{q(z^{(l)})} f(z^{(l)}) $$\n",
    "where the quantities $$r_l = \\frac{p(z^{(l)})}{q(z^{(l)})}$$ are called *importance weights* and correct the bias introduced by sampling from a proposal distribution instead of the correct one.\n",
    "\n",
    "Similarly to Rejection Sampling, in practice many times the value of $p(z)$ can be computed only up to a normalization constant $Z$: $$p(z) = \\frac{1}{Z_p} \\tilde{p}(z)$$ where $\\tilde{p}(z)$ can be easily evaluated, but $Z_p$ is unknown. We assume the same for the proposal distribution $q(z)$, which can be written as $$q(z) = \\frac{1}{Z_q} \\tilde{q}(z)$$\n",
    "\n",
    "We then proceed to rewrite the expectation in this form:\n",
    "$$ E[f] = ∫ f(z) p(z) dz = \\frac{Z_q}{Z_p}∫ f(z) \\frac{\\tilde{p}(z)}{\\tilde{q}(z)} q(z) dz \\simeq \\frac{Z_q}{Z_p} \\frac{1}{L} \\sum_{l=1}^{L} \\tilde{r}_l f(z^{(l)})$$\n",
    "where $$\\tilde{r}_l = \\frac{\\tilde{p}(z^{(l)})}{\\tilde{q}(z^{(l)})}$$\n",
    "\n",
    "If we also estimate the ratio $\\frac{Z_q}{Z_p}$ using the same samples we obtain\n",
    "$$\\frac{Z_q}{Z_p} = \\frac{1}{Z_q} ∫ \\tilde{p}(z) dz = ∫ \\frac{\\tilde{p}(z)}{\\tilde{q}(z)} dz \\simeq \\frac{1}{L} \\sum_{l=1}^{L} \\tilde{r}_l $$\n",
    "\n",
    "and then we can rewrite the final weighted average as:\n",
    "$$E[f] \\simeq \\sum_{l=1}^{L} w_l f(z^{(l)}) $$\n",
    "\n",
    "where $$w_l = \\frac{\\tilde{r}_l}{\\sum_{m} \\tilde{r}_m} = \\frac{\\tilde{p}(z^{(l)})/q(z^{(l)})}{\\sum_{m} \\tilde{p}(z^{(m)})/q(z^{(m)}) } $$\n",
    "\n",
    "Analogously to Rejection Sampling, the effectiveness of Importance Sampling strongly depends on how well the proposal distribution $q(z)$ matches the target distribution $p(z)$.\n",
    "\n",
    "When the function $p(z)f(z)$ varies significantly and is concentrated in narrow regions of the space, if the proposal distribution $q(z)$ assigns them little probability, most importance weights will be nearly zero, with only a few having large values. As a result, small number of samples will dominate, reducing the effective sample size, and in extreme cases none of the samples fall in the high-probability regions, resulting in very inaccurate estimates, even if the sample variance appears small. This represents the major weakness of Importance Sampling: it can give misleading results without any indication that the estimate may be unreliable. \n",
    "\n",
    "To avoid this, it’s essential that $q(z)$ is not too small in areas where \n",
    "$p(z)f(z)$ is large.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185978f-c98d-4812-9c02-4a7ab9e3e99c",
   "metadata": {},
   "source": [
    "#### In Practice\n",
    "The distributions $p(z)$ and $q(z)$, along with the function $f(z)$, used in this example are a close match of the ones shown in the figure present in Bishop's _Pattern Recognition and Machine Learning_ 's paragraph about Importance Sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc17ad35-fd5f-4b31-8d14-a6db6cfc860e",
   "metadata": {},
   "source": [
    "> ###### Note: \n",
    "> Since `importance_sampling` is a very simple function, we can improve its modularity by using `scipy.stats`'s `rvs` (see [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html)) to sample from the proposal $q(z)$ instead of manually sampling with `np.random.normal()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6265d6fa-0461-4dfa-a31b-04bbf4d0754d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163163a219494171b035c78773a17655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.4, description='μ', max=3.5, min=1.5, step=0.05), FloatSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_is(mu_q=2.4, sigma_q=0.75)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "\n",
    "# Bimodal target distribution p(z)\n",
    "def p(z):\n",
    "    return 0.6 * norm.pdf(z, loc=1.8, scale=0.3) + 0.4 * norm.pdf(z, loc=3.0, scale=0.3)\n",
    "\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-1.5 * (z - 2.4))) - 0.10\n",
    "\n",
    "def importance_sampling(f, p, q_dist, n_samples=10000):\n",
    "    samples = q_dist.rvs(size=n_samples)\n",
    "    \n",
    "    p_samples = p(samples)\n",
    "    q_samples = q_dist.pdf(samples)\n",
    "    weights = p_samples / q_samples\n",
    "    f_samples = f(samples)\n",
    "    \n",
    "    estimate = np.mean(f_samples * weights)\n",
    "    return estimate, samples, weights, f_samples\n",
    "\n",
    "# Plotting code generated by ChatGPT\n",
    "def plot_is(mu_q=2.4, sigma_q=0.75):\n",
    "    z = np.linspace(-1, 5, 1000)\n",
    "    q_dist = norm(loc=mu_q, scale=sigma_q)\n",
    "    \n",
    "    estimate, samples, weights, f_samples = importance_sampling(f, p, q_dist)\n",
    "    \n",
    "    p_z = p(z)\n",
    "    q_z = q_dist.pdf(z)\n",
    "    f_z = f(z)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(z, f_z, 'b', linewidth=2, label='f(z)')\n",
    "    plt.plot(z, p_z, 'r', linewidth=2, label='p(z) (Target)')\n",
    "    plt.plot(z, q_z, 'g', linewidth=2, label='q(z) (Proposal)')\n",
    "    plt.title(f\"Target vs Proposal\")\n",
    "    plt.xlabel(\"z\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    ax1.hist(samples, bins=50, weights=weights, density=True, alpha=0.6, label='Weighted Samples')\n",
    "    ax1.plot(z, p_z, 'r', linewidth=2, label='p(z) (Target)')\n",
    "    ax1.set_title(\"Importance Sampling\")\n",
    "    ax1.set_xlabel(\"z\")\n",
    "    ax1.set_ylabel(\"Density\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.hist(weights, bins=100, density=True, color='orange', alpha=0.7)\n",
    "    ax2.set_title(\"Distribution of Importance Weights\")\n",
    "    ax2.set_xlabel(\"Weight value\")\n",
    "    ax2.set_ylabel(\"Density\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "interact(\n",
    "    plot_is,\n",
    "    mu_q=FloatSlider(value=2.4, min=1.5, max=3.5, step=0.05, description='μ'),\n",
    "    sigma_q=FloatSlider(value=0.75, min=0.1, max=1.5, step=0.05, description='σ')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec6fe95-f68b-46e8-b034-643c1959b233",
   "metadata": {},
   "source": [
    "When $\\mu = 2.40$ and $\\sigma = 0.75$, the first plot matches the figure from Bishop’s book and shows the function $f(z)$ that we want to integrate, the target distribution $p(z)$ and the ideal choice of the proposal distribution $q(z)$ for this problem.\n",
    "\n",
    "The second plot compares the histogram of the samples, drawn from $q(z)$ and adjusted using importance weights, to the target distribution $p(z)$. Clearly, the sampling process is effective when the samples closely match $p(z)$: this is the case when $q(z)$ is defined accordingly to the example of the book. \n",
    "\n",
    "The third plot shows the distribution of the importance weights, which reflect how much each sample contributes to the final estimate. When $q(z) = N(\\mu = 2.40, \\sigma = 0.75)$, most weights have moderate values, with few weights presenting near-zero or high values: many samples contribute meaningfully to the result of Importance Sampling, reducing the variance of the estimator and leading to a more reliable estimation.\n",
    "\n",
    "By using the interactive widget to modify the mean $\\mu$ and standard deviation $\\sigma$ of the proposal distribution, we can observe how sensitive the method is to the choice of $q(z)$. \n",
    "\n",
    "If the proposal is centered away from one of the modes of $p(z)$, it generates fewer samples in that region, and a high number of weights, the great majority, will have near-zero values, i.e. they won't contribute to the final estimate, resulting in instability of the estimator.\n",
    "\n",
    "If the standard deviation $\\sigma$ of $q(z)$ is set too small, the proposal distribution becomes too narrow and does not cover the target distribution properly: many samples are drawn from irrelevant regions (where $p(z)≈0$), leading to many near-zero weights and a few large ones, and causing an increase in variance and the results to be unstable. However, if $\\sigma$ is too large, $q(z)$ becomes too flat and places too many samples in regions where both $p(z)≈0$ and $f(z)≈0$: again, this leads to most samples having low importance and the sampling's efficiency being poor.\n",
    "\n",
    "Ideally, the proposal should cover all the significant regions of $p(z)$ to ensure the weights stay moderate and the sampling remains efficient; in Bishop's case, the target $p(z)$ is bimodal, which results in the ideal positioning of $q(z)$ being in the center of its two modes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7357f0-a1d4-4389-9bd3-7a822b41a1bb",
   "metadata": {},
   "source": [
    "### 1.5 - Sampling-Importance-Resampling\n",
    "As discussed in Paragraph 1.2, Rejection Sampling's effectiveness relies on selecting a suitable constant $k$ such that $p(z) \\leq kq(z)$ $\\forall z$. However, finding it will be impractical for many pairs of distributions, especially when the required $k$ is large, because it will lead to a very low acceptance rate. \n",
    "\n",
    "The initial approach taken by SIR is similar to Rejection Sampling: it starts with a proposal distribution $q(z)$, but avoids the need to find a constant $k$.\n",
    "\n",
    "The SIR method consists in two stages:\n",
    "- *Sampling*: we draw L samples $z^{(1)}, ..., z^{(L)}$ from $q(z)$\n",
    "- *Weighting and Resampling*: we construct L weights $w_1, ..., w_L$ using $$w_l = \\frac{\\tilde{r}_l}{\\sum_{m} \\tilde{r}_m} = \\frac{\\tilde{p}(z^{(l)})/q(z^{(l)})}{\\sum_{m} \\tilde{p}(z^{(m)})/q(z^{(m)}) } $$ then a second set of L samples is drawn from the discrete distribution $z^{(1)}, ..., z^{(L)}$ with probabilities given by the weights $(w_1, ..., w_L)$\n",
    "\n",
    "The final set of resampled values only approximately follows the target distribution $p(z)$, but this approximation improves as $L→∞$, where  the cumulative distribution $p(z \\leq a)$ of the resampled values converges to that of $p(z)$. This can be shown by expressing the cumulative probability as a weighted sum of indicator functions and observing that, taking the limit $L→∞$, the sum becomes an integral weighted according to the original sampling distribution $q(z)$, matching the cumulative distribution function of $p(z)$.\n",
    "\n",
    "However, if we assume a finite value of $L$ and a given initial sample set, the resampled values will only be an appromixation of the values following the desired distribution. Again, as in the case of Rejection Sampling and Importance Sampling,  the approximation improves as the proposal distribution $q(z)$ gets closer to\n",
    "$p(z)$. When $q(z) = p(z)$, the initial samples $z^{(1)}, ..., z^{(L)}$ have the desired distribution, and all weights become uniform ($w_i = 1/L$) so that resampling has no effect and the resampled values also have the desired distribution.\n",
    "\n",
    "Additionally, if moments with respect to the distribution $p(z)$ are required, then they can be directly estimated from the original samples and their weights, without needing the resampled set, since $$ E[f] \\simeq \\sum_{l=1}^{L} w_l f(z_l) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827aa98-0421-46d3-9141-6c007b8e359c",
   "metadata": {},
   "source": [
    "#### In Practice\n",
    "This example can be considered as an extension of the previous one on Importance Sampling.  \n",
    "\n",
    "In addition to computing weighted samples from a proposal distribution using Importance Sampling, SIR adds a resampling step, in which a new set of samples is drawn according to the normalized importance weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac9d36e-6a75-4d99-bef5-96801e967579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d496bc802c9b473ca361f46c35b7c87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.4, description='μ', max=3.5, min=1.5, step=0.05), FloatSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_sir(mu_q=2.4, sigma_q=0.75, n_samples=10000)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from ipywidgets import interact, FloatSlider, IntSlider\n",
    "\n",
    "\n",
    "# Bimodal target distribution p(z)\n",
    "def p(z):\n",
    "    return 0.6 * norm.pdf(z, loc=1.8, scale=0.3) + 0.4 * norm.pdf(z, loc=3.0, scale=0.3)\n",
    "\n",
    "def f(z):\n",
    "    return 1 / (1 + np.exp(-1.5 * (z - 2.4))) - 0.10\n",
    "\n",
    "\n",
    "def sir(f, p, q_dist, n_samples=10000):\n",
    "    # Sampling according to Importance Sampling\n",
    "    estimate, samples, weights, f_samples = importance_sampling(f, p, q_dist, n_samples)\n",
    "\n",
    "    # Weight normalization\n",
    "    norm_weights = weights / np.sum(weights)\n",
    "    \n",
    "    # Resampling: choose between initial samplings with probability proportional to normalized weights\n",
    "    resampled_indices = np.random.choice(len(samples), size=n_samples, replace=True, p=norm_weights)\n",
    "    resampled_samples = samples[resampled_indices]\n",
    "    \n",
    "    return estimate, samples, weights, resampled_samples\n",
    "\n",
    "\n",
    "def plot_sir(mu_q=2.4, sigma_q=0.75, n_samples=10000):\n",
    "    z = np.linspace(-1, 5, 1000)\n",
    "    q_dist = norm(loc=mu_q, scale=sigma_q)\n",
    "    \n",
    "    estimate, samples, weights, resampled_samples = sir(f, p, q_dist, n_samples=n_samples)\n",
    "    \n",
    "    p_z = p(z)\n",
    "    q_z = q_dist.pdf(z)\n",
    "    f_z = f(z)\n",
    "    \n",
    "    # Plot Target, Proposal, and f(z)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(z, f_z, 'b', linewidth=2, label='f(z)')\n",
    "    plt.plot(z, p_z, 'r', linewidth=2, label='p(z) (Target)')\n",
    "    plt.plot(z, q_z, 'g', linewidth=2, label='q(z) (Proposal)')\n",
    "    plt.title(\"Target vs Proposal\")\n",
    "    plt.xlabel(\"z\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Weighted samples + resampled samples\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax1.hist(samples, bins=50, weights=weights, density=True, alpha=0.6, label='Weighted Samples')\n",
    "    ax1.plot(z, p_z, 'r', linewidth=2, label='p(z)')\n",
    "    ax1.set_title(\"Importance Sampling\")\n",
    "    ax1.set_xlabel(\"z\")\n",
    "    ax1.set_ylabel(\"Density\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.hist(resampled_samples, bins=50, density=True, alpha=0.6, color='orange', label='Resampled')\n",
    "    ax2.plot(z, p_z, 'r', linewidth=2, label='p(z)')\n",
    "    ax2.set_title(\"SIR\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Distribution of weights\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax1.hist(weights, bins=100, density=True, color='blue', alpha=0.7)\n",
    "    ax1.set_title(\"Distribution of Importance Weights\")\n",
    "    ax1.set_xlabel(\"Weight value\")\n",
    "    ax1.set_ylabel(\"Density\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    ax2.hist(weights / np.sum(weights), bins=50, density=True, color='purple', alpha=0.7)\n",
    "    ax2.set_title(\"Normalized Importance Weights\")\n",
    "    ax2.set_xlabel(\"Weight value\")\n",
    "    ax2.set_ylabel(\"Density\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "interact(\n",
    "    plot_sir,\n",
    "    mu_q=FloatSlider(value=2.4, min=1.5, max=3.5, step=0.05, description='μ'),\n",
    "    sigma_q=FloatSlider(value=0.75, min=0.1, max=1.5, step=0.05, description='σ'),\n",
    "    n_samples=IntSlider(value=10000, min=100, max=50000, step=100, description='# samples')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0a1b83-92d7-44db-8700-018a01fa87ff",
   "metadata": {},
   "source": [
    "The first plot and the plots on the left (and the logic behind them) are exactly the same as the previous example shown in the Importance Sampling paragraph.\n",
    "\n",
    "Instead, the first plot on the right shows the result of the SIR algorithm: compared to the Importance Sampling plot, it shows the resampled values. We can see that it provides a better approximation of the target distribution, especially as the number of samples increases.\n",
    "\n",
    "Finally, the third set of plots illustrates the distribution of the weights: on the left we see the raw importance weights, while on the right it shows the normalized weights, which sum to one and represent the probabilities used for the resampling step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d68988-5806-414b-a31f-1b697ff8639a",
   "metadata": {},
   "source": [
    "### 1.6 - Sampling and The EM Algorithm\n",
    "Monte Carlo sampling methods are not only useful in Bayesian analysis: they also assist in frequentist approaches. In particular, sampling methods can be used to approximate the E-step of the EM algorithm for models in which it cannot be performed analytically.\n",
    "\n",
    "#### The General EM Algorithm\n",
    "The Expectation-Maximization (EM) algorithm is a popular method for learning models with hidden (or latent) variables, used to find good parameter values for a probabilistic model when we have incomplete data.\n",
    "\n",
    "EM works by alternating between two steps:\n",
    "- The *E-step* (Expectation): estimate the missing or hidden parts of the data using the current model\n",
    "\n",
    "- The *M-step* (Maximization): update the model parameters to better fit the data, assuming the estimated hidden parts are correct\n",
    "\n",
    "This process repeats until the model stabilizes.\n",
    "\n",
    "Formally:\n",
    "\n",
    "> Given a joint distribution $p(X, Z|θ)$ over observed variables X and latent variables $Z$, governed by parameters $θ$, the goal is to maximize the likelihood function $p(X|θ)$ with respect to $θ$.\n",
    "> 1. Choose an initial setting for the parameters $θ^{old}$.\n",
    "> 2. *E-step*: Evaluate $p(Z|X, θ^{old})$.\n",
    "> 3. *M-step*: Evaluate $θ^{new}$ given by $$θ^{new} = \\arg \\max_{θ} Q(θ, θ^{old})$$\n",
    "where $$ Q(θ, θ^{old}) = ∫ p(Z|X, θ^{old}) \\ ln \\ p(X, Z|θ)\\ dZ$$\n",
    "> 4. Check for convergence of either the log likelihood or the parameter values.\n",
    "If the convergence criterion is not satisfied, then let $$θ^{old} ← θ^{new}$$\n",
    "and return to step 2.\n",
    "\n",
    "\n",
    "However, in many cases the integral in Step 2 is intractable, we can approximate it using sampling: we draw $L$ samples ${Z^{(l)}}$ from the current estimate for the posterior distribution $p(Z∣X,θ^{old})$, and estimate $Q$ as:\n",
    "$$ Q(θ, θ^{old}) \\simeq \\frac{1}{L} \\sum_{l=1}^{L} ln \\ p(Z^{(l)}∣X,θ)$$\n",
    "Once the approximation is computed, the M-step proceeds normally to update the parameters. This procedure is called the *Monte Carlo EM Algorithm*.\n",
    "\n",
    "#### Stochastic EM\n",
    "A particular case is Stochastic EM, where only one sample is drawn during each E-step. This is useful for mixture models, where the hidden variable $Z$ indicates which mixture component generated each data point.\n",
    " \n",
    "In this case:\n",
    "- The E-step draws a sample from the posterior $p(Z∣X,θ^{old})$, assigning each data point to a component in the mixture\n",
    "- The M-step uses this assignment to update the model parameters in the usual way\n",
    "\n",
    "#### IP Algorithm\n",
    "If we adopt a fully Bayesian perspective and aim to sample from the posterior distribution over the parameter vector $θ$, we ideally would like to draw samples from the joint posterior $p(θ, Z∣X)$. Suppose that this is computationally hard and that sampling from the complete-data parameter posterior $p(θ|Z, X)$ is relatively straightforward: we can use the *Data Augmentation* algorithm, also known as the IP (Imputation–Posterior) algorithm, which alternates between two steps, I and P, respectively analogous to the E-step and M-step, consisting in:\n",
    "\n",
    "- *I-step*:  we wish to sample from $p(Z|X)$ but can't do it directly. But we can use the following relation: $$p(Z|X) = ∫ p(Z|θ, X)p(θ|X) dθ$$ to draw a sample $θ^{(l)}$ from the current estimate for $p(θ|X)$ and then use it to draw a sample $Z^{(l)}$ from $p(Z|θ^{(l)}, X) \\  \\forall \\ l \\in L$\n",
    "  \n",
    "- *P-step*: given the relation $$p(θ|X) = ∫ p(θ|Z, X)p(Z|X) dZ$$ we use the samples ${Z^{(l)}}$ obtained from the I-step to approximate the posterior over $θ$ as $$p(θ∣X) \\simeq \\frac{1}{L} \\sum_{l=1}^{L} p(θ|Z^{(l)}, X)$$ from which we can sample, by assumption, in the next iteration.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdfa4a9-99f4-4ada-b727-e98b778ec9c7",
   "metadata": {},
   "source": [
    "## 2 - Markov Chain Monte Carlo\n",
    "\n",
    "In the previous section, we discussed Rejection Sampling and Importance Sampling as methods for estimating expectations  of functions under a given probability distribution. However, both techniques tend to fail in high-dimensional spaces, where they become highly inefficient.\n",
    "\n",
    "We now introduce a more flexible and powerful family of methods called *Markov Chain Monte Carlo (MCMC)*, which allow sampling from a large class of distributions and scale much better with dimensionality.\n",
    "\n",
    "Again, as with Rejection and Importance Sampling, MCMC also uses a *proposal distribution* $q(\\cdot)$ to generate samples. But this time, instead of sampling independently each time, we mantain a record of the current state $z^{(τ)}$: the proposal function $q(z|z^{(τ)})$ depends on this current state, resulting in a sequence of *dependent samples*, where the next sample depends on the current one. This new type of sequence, commonly known as *Markov chain*, has the property that each new state depends only on the current state $z^{(τ)}$, not on the full history of precedent states.\n",
    "\n",
    "As before, if we write $p(z) = \\tilde{p}(z) / Z_p$, we assume that $\\tilde{p}(z)$ can easily be evaluated for any value of $z$ and that it isn't necessary to know the value of $Z_p$.  At each iteration of the algorithm, we generate a candidate sample $z^*$ based on the current state $z^{(τ)}$ from the proposal distribution $q(z^*|z^{(τ)})$, assuming that it is chosen easy to sample from, and then decide whether to accept or reject a sample according to an appropriate criterion.\n",
    "\n",
    "#### The Metropolis Algorithm\n",
    "A classical first example of a MCMC method is the Metropolis Algorithm, which works under the assumption that the proposal distribution is symmetric, i.e. $q(z_A|z_B) = q(z_B|z_A) \\  \\forall z_A, z_B$.\n",
    "\n",
    "This symmetry simplifies the acceptance rule of the candidate sample, which is accepted with probability equal to:\n",
    "\n",
    "$$ A(z^*,z^{(τ)}) = \\min (1, \\frac{\\tilde{p}(z^*)}{\\tilde{p}(z^{(τ)})}) $$\n",
    "\n",
    "This can be achieved by drawing a random number $u∼Uniform(0,1)$ and by accepting the sample if $ A(z^*,z^{(τ)}) > u $. This means that if the candidate sample has a higher probability under $p(z)$ than the current one, it will always be accepted, but if it’s lower it still has a chance to be accepted. \n",
    "\n",
    "If the candidate sample is accepted, we set $z^{(τ+1)} = z^*$, otherwise we keep the current state, discard $z^*$, set $z^{(τ+1)} = z^{(τ)}$, and draw another candidate sample from the distribution $q(z|z^{(τ+1)})$. Note that this is different from Rejection Sampling, where we simply discarded the rejected sample: the Metropolis algorithm repeats the previous state and, as a result, the final sequence may contain repeated values. In practice, we only keep a single copy of each retained sample, along with a weight recording how many times that state appears. \n",
    "\n",
    "It is sufficient (but not necessary) that $q(z_A|z_B) > 0$ holds $\\forall z_A, z_b$ for the chain to eventually produce samples $z^{(τ)}$ that approximate the true distribution $p(z)$, as the number of steps increases (i.e., as $τ→∞$). However, we should note that the sequence $z^{(1)}, z^{(2)}, ...$ is not composed of independent samples: each sample is highly correlated to the previous one. A common practice to obtain independent samples is to thin the chain by only keeping every $M$-th sample, where $M$ is large enough, and discarding all the others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91749d2-c6bb-40f6-a0c1-2353c6acc573",
   "metadata": {},
   "source": [
    "#### In Practice\n",
    "See Paragraph 2.2's \"In Practice\" below for an implementation of the example in figure 11.9 of Bishop's book. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186636e5-ca19-428e-9c76-1f300889ba4f",
   "metadata": {},
   "source": [
    "### 2.1 - Markov Chains\n",
    "Before introducing the Metropolis-Hastings Algorithm, it's useful to first understand the general behavior of Markov chains, especially the conditions under which they converge to a target distribution. In the precedent paragraph, we defined a first-order Markov chain as a sequence of random variables $z^{(1)}, ... , z^{(M)}$ where each variable depends only on its immediate predecessor, which formally translates to: $$ p(z^{(m+1)} | z^{(1)}, ... , z^{(m)}) = p(z^{(m+1)} | z^{(m)}) \\ \\forall  \\ m \\in \\{ 1, ... , M - 1 \\} $$\n",
    "\n",
    "This structure can be visualized as a directed graph in the form of a chain which is fully specified by the probability distribution for the initial variable $p(z^{(0)})$ and its *transition probabilities* $T_m(z^{(m)}, z^{(m+1)}) \\equiv p(z^{(m+1)}| z^{(m)})$, i.e. the conditional probabilities for subsequent variables. If these transition probabilities remain constant over time, the chain is called *homogeneous*.\n",
    "\n",
    "The marginal probability of each state in the chain can be computed iteratively from the previous state using: $$p(z^{(m+1)}) = \\sum_{z^{(m)}} p(z^{(m+1)}| z^{(m)}) p(z^{(m)}) $$\n",
    "\n",
    "A distribution is called *invariant* (or stationary) with respect to a Markov chain if it does not change when we apply the transition rules. So, for a homogeneous chain with transition probabilities $T(z',z)$, a distribution $p^*(z)$ is invariant if:\n",
    "$$ p^*(z) = \\sum_{z'} T(z',z) p^*(z') $$\n",
    "Note that a given Markov chain might have multiple invariant distributions: for example, if the chain never changes state (identity transitions), then any distribution will be invariant.\n",
    "\n",
    "To ensure that a distribution $p(z)$ is invariant, one sufficient (but not necessary) condition is to satisfy the *detailed balance* property:\n",
    "$$ p^*(z) T(z,z') = p^*(z') T(z',z)$$ \n",
    "If this condition is met, then the chain will leave the distribution $p^*(z)$ unchanged. A Markov chain that respects detailed balance is said to be *reversible*.\n",
    "\n",
    "Our goal is to use Markov chains to sample from a given distribution. In addition to the desired distribution being invariant, we also require the chain to converge to this distribution regardless of the initial state: this convergence property is known as *ergodicity*,  and the invariant distribution is then called the *equilibrium distribution*. Note that an ergodic Markov chain has only one equilibrium distribution and that most homogeneous Markov chains will be ergodic, as the invariant distribution and the transition probabilities are only needed to meet weak conditions.\n",
    "\n",
    "In practice, we often construct transition probabilities as combinations of simpler \"base\" transitions $B_1, ..., B_K$. This combination can be a mixture: \n",
    "$$ T(z',z) = \\sum_{k=1}^{K} \\alpha_k B_k(z',z) $$ where $\\alpha_1, ..., \\alpha_K$ are non-negative weights and $\\sum_{k} \\alpha_k = 1$, or a sequence of transitions:\n",
    "$$T(z',z) = \\sum_{z_1} ...  \\sum_{z_{K-1}} B_1(z',z_1) ... B_{K-1}(z_{K-2},z_{K-1})B_K(z_{K-1},z)$$\n",
    "If a distribution is invariant under each base transition, then it will also be invariant under the combined transition. In the mixture case, if each $B_K$ satisfies detailed balance, then $T$ does too. This does not hold for sequential transitions, which detailed balance can be preserved by applying transitions symmetrically ($B_1, B_2, ..., B_K, B_K, ..., B_2, B_1$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470499d-64b7-4537-a66f-9bb534e1a9a6",
   "metadata": {},
   "source": [
    "### 2.2 - The Metropolis-Hastings Algorithm\n",
    "\n",
    "We can generalize the Metropolis Algorithm, which was introduced in Paragraph 2, to the case where the proposal distribution is no longer a symmetric function of its arguments.\n",
    "\n",
    "At each step $τ$, in which the current state is $z^{(τ)}$, a new candidate state $z^*$ is sampled from a proposal distribution $q_k(z∣z^{(τ)})$. This move is then accepted with probability:\n",
    "$$ A_k(z^*,z^{(τ)}) = \\min (1, \\frac{\\tilde{p}(z^*)q_k(z^{(τ)}|z^*)}{\\tilde{p}(z^{(τ)})q_k(z^*,z^{(τ)})}) $$\n",
    "where $k$ represents the elements of the set of possible transitions being considered. Again, the normalization constant $Z_p$ of the target distribution $p(z) = \\tilde{p}(z) / Z_p$ is not required, since it cancels out in the acceptance ratio. If the proposal distribution is symmetric, this formula simplifies back to the original Metropolis rule.\n",
    "\n",
    "We can show that $p(z)$ is an invariant distribution of the Markov chain defined\n",
    "by the Metropolis-Hastings algorithm by showing that detailed balance property is satisfied:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(z)q_k(z|z')A_k(z',z) &= \\min \\left(p(z)q_k(z|z'),\\ p(z')q_k(z'|z)\\right) \\\\\n",
    "                       &= \\min \\left(p(z')q_k(z'|z),\\ p(z)q_k(z|z')\\right) \\\\\n",
    "                       &= p(z')q_k(z'|z)A_k(z, z')\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The choice of proposal distribution significantly affects the algorithm’s performance. For continuous variables, a common choice is a *Gaussian* centered at the current state, but determining its variance parameter requires a trade-off. If the variance is too small most proposed states are accepted, but the chain moves slowly and it leads to high correlation between samples; however, if the variance is too large, many proposed steps will be to states for which the probability $p(z)$ is low, leading to a high rejection rate and resulting in slow convergence.\n",
    "\n",
    "This trade-off becomes especially evident in high-dimensional problems with correlated variables. To optimize performance, the scale $𝜌$ of the proposal distribution should be of the same order as the smallest length scale $𝜎_{min}$ in the target distribution, i.e. big enough to explore the space efficiently, but not enough to result in high rejection rates. For instance, the time to generate approximately independent samples is of order $(𝜎_{max} /𝜎_{min}⁡)^2$, where $𝜎_{max}$ and $𝜎_{min}$ are the largest and smallest scales in the target distribution. \n",
    "\n",
    "In general, if the target distribution has very different scales in different directions, i.e. it varies a lot in different directions, Metropolis-Hastings might converge very slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a81914-0ff1-4bb7-8975-e389fa3f9848",
   "metadata": {},
   "source": [
    "#### In Practice\n",
    "\n",
    "Following Bishop's example, we will implement the Metropolis algorithm choosing a bivariate Gaussian distribution as our target. \n",
    "\n",
    "Since the Metropolis algorithm is a special case of Metropolis-Hastings, we will only write a general Metropolis-Hastings implementation, which can be used to demonstrate both cases: a symmetric proposal $q$ (corresponding to the classic Metropolis algorithm), and an asymmetric proposal $q$ (Metropolis-Hastings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a91dbc58-dfcd-403e-aa9e-751b4119b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "# Target: bivariate Gaussian chosen to resemble the example in fig. 11.9 - Bishop\n",
    "mu = np.array([0, 0])\n",
    "Sigma = np.array([[1, 0.8], \n",
    "                  [0.8, 1]])\n",
    "target = multivariate_normal(mean=mu, cov=Sigma)\n",
    "\n",
    "\n",
    "\n",
    "def metropolis_hastings(p, q, q_density, z_0, n_samples=500):\n",
    "    samples = [z_0]\n",
    "    proposals = []\n",
    "    z_current = z_0\n",
    "\n",
    "    for _ in range(n_samples):\n",
    "        z_proposed = q(z_current)\n",
    "\n",
    "        proposals.append(z_proposed)\n",
    "\n",
    "    \t# If q is symmetric, alpha will be simplified to p(z_proposed) / p(z_current)\n",
    "        alpha =  (\n",
    "            p(z_proposed) * q_density(z_current, z_proposed)\n",
    "            / (p(z_current) * q_density(z_proposed, z_current))\n",
    "        )\n",
    " \n",
    "\n",
    "        alpha = min(1, alpha)\n",
    "\n",
    "        u = np.random.rand()\n",
    "        if u < alpha:\n",
    "            z_current = z_proposed\n",
    "\n",
    "        samples.append(z_current)\n",
    "\n",
    "    # Convert list to np.array for easier handling\t\n",
    "    return np.array(samples), np.array(proposals)\n",
    "\n",
    "\n",
    "def plot_metropolis_hastings(p, q,q_density, n_samples=500, z0_choice=\"(-3,-3)\"):\n",
    "    \n",
    "    # Mapping for starting points\n",
    "    z0_map = {\n",
    "        \"(-3,-3)\": np.array([-3.0, -3.0]),\n",
    "        \"(-3,3)\": np.array([-3.0, 3.0]),\n",
    "        \"(3,-3)\": np.array([3.0, -3.0]),\n",
    "        \"(3,3)\": np.array([3.0, 3.0]),\n",
    "    }\n",
    "    z_0 = z0_map[z0_choice]\n",
    "\n",
    "\n",
    "    # Run sampler\n",
    "    samples, proposals = metropolis_hastings(p, q, q_density, z_0, n_samples)\n",
    "\n",
    "\n",
    "    # Plotting code below generated by ChatGPT\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    # Ellipse of the target Gaussian\t\n",
    "    x, y = np.mgrid[-4:4:.01, -4:4:.01]\n",
    "    pos = np.dstack((x, y))\n",
    "    ax.contour(x, y, target.pdf(pos), levels=1, colors='k', alpha=0.5)\n",
    "\n",
    "    # Draw path\n",
    "    for i in range(len(proposals)):\n",
    "        start, end = samples[i], proposals[i]\n",
    "        accepted = np.allclose(samples[i+1], end)\n",
    "        if accepted:\n",
    "        \tax.plot([start[0], end[0]], [start[1], end[1]], color='g', linewidth=1, alpha=0.8)\n",
    "        else:\n",
    "        \tax.plot([start[0], end[0]], [start[1], end[1]], color='r', linewidth=1, alpha=0.3)\n",
    "\n",
    "\n",
    "    ax.set_title(f\"Metropolis-Hastings\")\n",
    "    ax.set_xlim(-4, 4)\n",
    "    ax.set_ylim(-4, 4)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607cadc2-e741-4830-bb37-ff89e11e3db4",
   "metadata": {},
   "source": [
    "##### Symmetric proposal function (Metropolis Algorithm)\n",
    "Following Bishop's example, $q$ is an isotropic Gaussian with standard deviation equal to $0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "824feb54-5e2c-43fc-a40c-b5cf193c048c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8bbdb26ab84e04bffa540d1b6030ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=500, description='# samples', max=1500, min=10, step=10), Dropdown(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_metropolis_hastings(p, q, q_density, n_samples=500, z0_choice='(-3,-3)')>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import fixed\n",
    "\n",
    "# Symmetric proposal: isotropic Gaussian with std deviation of 0.2 as in Bishop's example\n",
    "def q(z_current, sigma=0.2):\n",
    "    return z_current + np.random.normal(scale=sigma, size=2)\n",
    " \n",
    "def q_pdf(z_next, z_current, sigma=0.2):\n",
    "    return multivariate_normal.pdf(\n",
    "        z_next, mean=z_current, cov=sigma**2 * np.eye(2)\n",
    "    )\n",
    "\n",
    "\n",
    "# Interactive widget generated by ChatGPT\n",
    "interact(\n",
    "    plot_metropolis_hastings,\n",
    "    p=fixed(target.pdf),\n",
    "    q=fixed(q),\n",
    "    q_density=fixed(q_pdf),\n",
    "    n_samples=widgets.IntSlider(min=10, max=1500, step=10, value=500, description='# samples'),\n",
    "    z0_choice=widgets.Dropdown(\n",
    "        options=[\"(-3,-3)\", \"(-3,3)\", \"(3,-3)\", \"(3,3)\"],\n",
    "        value=\"(-3,-3)\",\n",
    "        description=\"z_0:\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9876dd99-b05c-4bf4-a19f-35e3f52b65f3",
   "metadata": {},
   "source": [
    "The resulting figure shows the trajectory of a Metropolis sampler applied to a two-dimensional Gaussian target distribution with mean $(1.5, 1.5)$, whose one standard-deviation contour is shown by the ellipse (as we already said, the proposal distribution is an isotropic Gaussian distribution whose standard deviation is $0.2$). \n",
    "\n",
    "Accepted proposed steps correspond to green lines, where rejected proposals are shown in red. As expected, many of the rejected steps are towards regions of lower probability density (i.e. outside the ellipse), while most accepted steps are closer or within the high-density region. \n",
    "\n",
    "If we increase the number of samples, the accepted steps gradually form a cloud of points that approximates the target Gaussian and the chosen starting point becomes less influent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375ac10c-59df-4c08-8664-589922c68b46",
   "metadata": {},
   "source": [
    "##### Asymmetric proposal function (Metropolis-Hastings Algorithm)\n",
    "In the asymmetric case, we choose $q$ as a Gaussian shifted by a fixed drift of\n",
    "$0.5$ along the $x$-axis and with standard deviation equal to $0.5$. Thus, the symmetry of the proposal is broken: $\\exists \\, (z_A, z_B) \\quad : \\quad q(z_A|z_B) \\neq q(z_B|z_A)$ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2232aaa-7b29-4cc5-a9aa-c31d42086661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31da27fb72074e4b8198dfa9c198bea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=800, description='# samples', max=1500, min=10, step=10), Dropdown(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_metropolis_hastings(p, q, q_density, n_samples=500, z0_choice='(-3,-3)')>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asymmetric proposal: Gaussian with drift\n",
    "def q(z_current, drift=np.array([0.5, 0]), sigma=0.5):\n",
    "    return np.random.multivariate_normal(mean=z_current + drift, cov=sigma**2 * np.eye(2)) \n",
    "\n",
    "def q_pdf(z_next, z_current, drift=np.array([0.5, 0]), sigma=0.5):\n",
    "    return multivariate_normal.pdf(z_next, mean=z_current + drift, cov=sigma**2 * np.eye(2))\n",
    "\n",
    "\n",
    "# Interactive widget generated by ChatGPT\n",
    "interact(\n",
    "    plot_metropolis_hastings,\n",
    "    p=fixed(target.pdf),\n",
    "    q=fixed(q),\n",
    "    q_density=fixed(q_pdf),\n",
    "    n_samples=widgets.IntSlider(min=10, max=1500, step=10, value=800, description='# samples'),\n",
    "    z0_choice=widgets.Dropdown(\n",
    "        options=[\"(-3,-3)\", \"(-3,3)\", \"(3,-3)\", \"(3,3)\"],\n",
    "        value=\"(-3,-3)\",\n",
    "        description=\"z_0:\"\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996e65d2-80f0-468e-9d60-d9f75d27e6a3",
   "metadata": {},
   "source": [
    "As before, accepted proposals are shown in green and rejected ones in red.\n",
    "\n",
    "We can easily notice that, with the same number of steps, the number of rejected proposals is significantly larger compared to the symmetric case, and most rejected steps are towards the right, consistently with the drift of the proposal distribution, which pushes the chain in that direction.\n",
    "\n",
    "However, as the number of samples increases, analogously to the symmetric case, the approximation of the target Gaussian gets more accurate and less influenced by the chosen starting point $z_0$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cefb413-0272-4d86-85a2-427122226b4e",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "We explored several sampling methods, starting from the theory and successively implementing interactive examples, following Bishop's treatment of the topic (contained in chapter 11 of the book). \n",
    "\n",
    "In summary, we learnt about the following methods:\n",
    "\n",
    "- Inverse Transform Sampling: suited for standard distributions, where the CDF is known or can be numerically inverted.\n",
    "\n",
    "- Rejection Sampling: useful for arbitrary and more complex distributions, but the efficiency strongly depends on the choice of the proposal distribution $q$ and the scaling constant $k$ .\n",
    "\n",
    "- Adaptive Rejection Sampling: designed for log-concave target distributions, it exploits their log-concavity to build an adaptive proposal, making it more efficient compared to classic Rejection Sampling.\n",
    "\n",
    "- Importance Sampling: estimates expectations under a target distribution by reweighting samples drawn from a simpler (thus more convenient) proposal distribution.\n",
    "\n",
    "- Sampling-Importance-Resampling: adds a resampling step to Importance Sampling to improve the accuracy of the approximation.\n",
    "\n",
    "- Metropolis-Hastings: general Markov Chain Monte Carlo method, useful to work with complex distributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (FML)",
   "language": "python",
   "name": "fml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
